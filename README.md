# BigDataProject

Authors: 
- **Liubomyr Mokrytskyi**
- **Ivan Nickolaichenko**

## üìù Description

_In this project our goal was to get stream data from crypto exchange BitMEX using websocket, transform and filter those data using Apache Spark Stream and save into the Cassandra Database, so after user can using API Gateway get summarized information about cryptocurrencies from it._

## System Design

![system-design.png](Photos%2Fsystem-design.png)

**BitMEX** - –∫—Ä–∏–ø—Ç–æ–±—ñ—Ä–∂–∞, –∑ –∫–æ—Ç—Ä–æ—ó –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Web Socket, –º–∏ –±—É–¥–µ–º–æ –±—É–¥—É–≤–∞—Ç–∏ –ø–æ—Ç—ñ–∫ –ø–æ—Å—Ç—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö, –ø—Ä–æ –ø–µ–≤–Ω—ñ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏, –∞–±–∏ –ø–æ—Ç—ñ–º —ó—Ö –æ–±—Ä–æ–±–ª—è—Ç–∏ —Ç–∞ –ø–æ–∫–∞–∑—É–≤–∞—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ –ø–µ–≤–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –Ω—ñ–π.

**USER** - —Ü–µ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –∑–∞—Å—Ç–æ—Å—É–Ω–∫—É, —è–∫–∏–π –º–æ–∂–µ –Ω–∞–¥—Å–∏–ª–∞—Ç–∏ –∑–∞–ø–∏—Ç–∏ –¥–æ API GATEWAY, –∞–±–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ—Ä–∏—Å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Å–∏—Ç—É–∞—Ü—ñ—é, —è–∫–∞ –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –Ω–∞ –∫—Ä–∏–ø—Ç–æ-–±—ñ—Ä–∂—ñ BitMEX –∑ –¥–µ—è–∫–∏–º–∏ –º–æ–Ω–µ—Ç–∞–º–∏, —è–∫—ñ –±—É–ª–∏ –≤–∏–±—Ä–∞–Ω—ñ –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å –Ω–∞–ø–µ—Ä–µ–¥. 

**Attention!**
–î–ª—è –¥–∞–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç—É –º–∏ –æ–±—Ä–∞–ª–∏ 4 –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏, —è–∫—ñ –±—É–¥—É—Ç—å –æ–±—Ä–æ–±–ª—è—Ç–∏—Å—å –æ–±–æ–≤‚Äô—è–∑–∫–æ–≤–∏–º–∏ –µ–Ω–¥–ø–æ—ñ–Ω—Ç–∞–º–∏, –∫–æ—Ç—Ä—ñ –±—É–ª–∏ –∑–≥–∞–¥–∞–Ω—ñ –≤ —É–º–æ–≤—ñ –∑–∞–≤–¥–∞–Ω—å - XBTUSD, ETHUSD, SOLUSD, DOGEUSD.

**API GATEWAY** - –ø—Ä–∏–π–º–∞—î —É—Å—ñ –∑–∞–ø–∏—Ç–∏, –∫–æ—Ç—Ä—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –Ω–∞–¥—Å–∏–ª–∞—î –¥–∞–Ω–æ–º—É —Å–µ—Ä–≤—ñ—Å—É, –Ω–∞–¥—Å–∏–ª–∞—î —ó—Ö –Ω–∞ –æ–±—Ä–æ–±–∫—É, –∞ –ø—ñ—Å–ª—è —Ç–æ–≥–æ, —è–∫ –±—É–¥–µ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π –∑–∞–ø–∏—Ç, –Ω–∞–¥—Å–∏–ª–∞—î –Ω–∞–∑–∞–¥ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É. –¢–æ–±—Ç–æ –ø–æ —Ñ–∞–∫—Ç—É —Ü–µ —á–∞—Å—Ç–∏–Ω–∞ —Å–∏—Å—Ç–µ–º–∏, —É —è–∫—ñ–π –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –≤—Å—è –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è (requests —ñ responses).

**Crypto Service** - –æ–ø—ñ—Å–ª—è —Ç–æ–≥–æ, —è–∫ –∑–∞–ø–∏—Ç –Ω–∞–¥—ñ–π—à–æ–≤ –¥–æ API GATEWAY, –≤—ñ–Ω ‚Äú–∫–∞–∂–µ‚Äù, —â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ–≤–µ—Ä–Ω—É—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —ñ –¥–∞–Ω–∏–π —Å–µ—Ä–≤—ñ—Å –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—Ö queries —Ä–æ–±–∏—Ç—å –∑–≤–µ—Ä–Ω–µ–Ω–Ω—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö StreamDatabase, —è–∫–∞ –º—ñ—Å—Ç–∏—Ç—å —É—Å—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –ø–æ—Ç–æ–∫—É –¥–∞–Ω–∏—Ö –±—ñ—Ä–∂—ñ, —Ç–∞ –¥—ñ—Å—Ç–∞—î —É—Å—é—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é, —è–∫–∞ —î –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—é. –†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∑ –¥–æ–ø–æ–º–æ–≥–æ—é –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ —Ç–∞ REST API. 

**StreamDatabase** - NoSQL –±–∞–∑–∞ –¥–∞–Ω–∏—Ö, —è–∫–∞ –∑–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω—ñ, –æ–±—Ä–æ–±–ª–µ–Ω—ñ (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–æ–≤–∞–Ω—ñ) –¥–∞–Ω—ñ, —è–∫—ñ –Ω–∞–¥—Ö–æ–¥—è—Ç—å –∑ –ø–æ—Ç–æ–∫—É –±—ñ—Ä–∂—ñ BitMEX, –ø—Ä–æ –≤–∏–∑–Ω–∞—á–µ–Ω—ñ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥ —ó—Ö–Ω—ñ sell/buy —Ü—ñ–Ω–∏, –æ–±‚Äô—î–º —Ç–æ—Ä–≥—ñ–≤–ª—ñ –∑–∞ –ø–µ–≤–Ω–∏–π –ø–µ—Ä—ñ–æ–¥ —á–∞—Å—É —ñ —Ç–¥. –ú–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏ Apache Cassandra, –æ—Å–∫—ñ–ª—å–∫–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö, —è–∫–∞ –±—É–¥–µ –Ω–∞–¥—Ö–æ–¥–∏—Ç–∏ –¥–æ —Å–∏—Å—Ç–µ–º–∏ —î –Ω–∞–¥–∑–≤–∏—á–∞–π–Ω–æ –≤–µ–ª–∏–∫–æ—é, —ó—ó –ø–æ—Ç—Ä—ñ–±–Ω–æ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–æ–±–ª—è—Ç–∏ —Ç–∞ –≤ –ø–æ–¥–∞–ª—å—à–æ–º—É —à–≤–∏–¥–∫–æ —á–∏—Ç–∞—Ç–∏, –∞ —è–∫ –º–∏ –∑–Ω–∞—î–º–æ —Ü–µ —ñ —î –æ—Å–Ω–æ–≤–Ω–∏–º–∏ –ø–µ—Ä–µ–≤–∞–≥–∞–º–∏ Cassandra.

**DataETL** - —Å–µ—Ä–≤—ñ—Å, —è–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –∑–∞ ETL –ø—Ä–æ—Ü–µ—Å —Ç–∞ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤:

- **Web socket** - —Ü–µ –ø–æ—á–∞—Ç–æ–∫ –ø–µ—Ä—à–æ–≥–æ –µ—Ç–∞–ø—É - Extraction. –ê–±–∏ –∫–æ–∂–µ–Ω —Ä–∞–∑, –∞–±–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ —è–∫—É—Å—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø–µ–≤–Ω—É –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É, –Ω–µ —Ä–æ–±–∏—Ç–∏ –Ω–æ–≤–∏–π request –∑–∞–ø–∏—Ç –¥–æ –Ω–∞—à–æ—ó –∫—Ä–∏–ø—Ç–æ-–±—ñ—Ä–∂—ñ BitMex, –º–∏ —Ä–æ–±–∏–º–æ –ø–µ–≤–Ω–∏–π connection –∑ –Ω–µ—é, —Ç–∞ –±–µ–∑–ø–µ—Ä–µ—Å—Ç–∞–Ω–∫—É –æ—Ç—Ä–∏–º—É—î–º–æ —Ü—ñ –¥–∞–Ω—ñ, —è–∫ –ø–æ—Ç—ñ–∫. –†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é WebSocket.
- **Queue** - –æ—Å–∫—ñ–ª—å–∫–∏ –¥–∞–Ω—ñ –º–æ–∂—É—Ç—å –Ω–∞–¥—Ö–æ–¥–∏—Ç–∏ –≤ —Å–∏—Å—Ç–µ–º—É –∑ —Ä—ñ–∑–Ω–∏–º–∏ —ñ–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏ —ñ –∫—ñ–ª—å–∫—ñ—Å—Ç—é, –Ω–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑—Ä–æ–±–∏—Ç–∏ —Ü–µ–π –ø–æ—Ç—ñ–∫ –∑—Ä—É—á–Ω–∏–º –¥–ª—è –æ–±—Ä–æ–±–∫–∏, —Ç–æ–±—Ç–æ —Å—Ç–∞–±—ñ–ª—å–Ω–∏–º, —Ç–æ–º—É –º–∏ —Ö–æ—á–µ–º–æ —á—ñ—Ç–∫–æ –≤–∏—à–∏–∫—É–≤–∞—Ç–∏ –∫–æ–∂–Ω—É –æ–¥–∏–Ω–∏—Ü—é –¥–∞–Ω–∏—Ö, –∞–±–∏ –Ω–µ –≤–∏–Ω–∏–∫–∞–ª–æ –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ —ó—ó –æ–±—Ä–æ–±—Ü—ñ. –†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Apache Kafka.
- **Transform/Load** - –Ω–∞—Å—Ç—É–ø–Ω—ñ –µ—Ç–∞–ø–∏ ETL –ø—Ä–æ—Ü–µ—Å—É, —è–∫—ñ —á–∏—Ç–∞—é—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ —á–µ—Ä–≥–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å Apache Kafka —Ç–∞ –≤–∏–∫–æ–Ω—É—é—Ç—å –Ω–µ–æ–±—Ö—ñ–¥–Ω—É —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—é (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ –æ–±–∏—Ä–∞—î —Ç—ñ–ª—å–∫–∏ —Ç—Ä–µ–π–¥–∏) —Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—é (—Ä–∞—Ö—É—î –∑–∞–≥–∞–ª—å–Ω–∏–π volume –¥–ª—è –ø–µ–≤–Ω–æ—ó –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏ –ø—Ä–æ—Ç—è–≥–æ–º –≥–æ–¥–∏–Ω–∏) –¥–∞–Ω–∏—Ö. –ü—ñ—Å–ª—è —á–æ–≥–æ –∑–±–µ—Ä—ñ–≥–∞—î —ó—Ö —É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ —Ç–∞–±–ª–∏—á–∫–∏ –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö Cassandra. –†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Apache Spark Streaming.

## üñ• Files

The result of this homework is:

- [commands.txt](commands.txt) - file that will help you, as a new user to quickly copy all necessary commands in the correct order. 
- [Photos](Photoes) - folder that contains all screenshots of application work results and photos for system design.
- [Design.drawio](Design.drawio) - file where the design of the system is stored
- [docker-compose.yaml](docker-compose.yaml) - file that will help you to create all necessary containers of helper services as cassandra, spark etc.
- [DataExtraction](DataExtraction) - folder in which files for ETL are stored.
    - [ddl.cql](DataExtraction/ddl.cql) - file that contains all necessary DDL queries for Cassandra database the project is needed.
    - [requirements.txt](DataExtraction/requirements.txt) - file with necessary Python libraries for the DataETL container
    - [create_tables.sh](DataExtraction/create_tables.sh) - bash file to create tables in cassandra
    - [socket_connect.py](DataExtraction/socket_connect.py) - Python file with web socket and loading data into the Apache Kafka queue.
    - [transform_1.py](DataExtraction/transform_1.py) - Python file to transform and filter data from the queue using Apache Spark Streaming and saving into the Cassandra database.
    - [Dockerfile](DataExtraction/Dockerfile) - Dockerfile using which you can create and start a container for ETL process. 
- [RestAPI](RestAPI) - folder in which files for REST API are stored:
    - [requirements_api.txt](RestAPI/requirements_api.txt) - file with necessary Python libraries for the REST API container
    - [start_api.sh](RestAPI/start_api.sh) - bash file to build REST API container
    - [kill_api.sh](RestAPI/kill_api.sh) - bash file to stop and delete REST API container
    - MVC - files:
      - [controller.py](RestAPI/controller.py) - flask app controller
      - [service.py](RestAPI/service.py)
      - [repository.py](RestAPI/repository.py)
    - [Dockerfile.api](RestAPI/Dockerfile.api) - Dockerfile using which you can create and start a container for REST API. 


## üñ• Usage

### How to run scripts

üîª **EXAMPLE BELOW** üîª

1. Clone the repository
2. Open powershell terminal
3. Enter the repository using the path to its local directory on your computer
4. Run next command: `docker-compose build` - to load and build Docker Images, create network
5. Run next command: `docker-compose up -d` - start installation in detached mode.
6. Run next command: `.\DataExtraction\create_tables.sh` - to create tables in the Cassandra Database
7. Make sure to run the insert queries from the `commands.txt` file
7. Open new terminal and run:
    - Run next command to create a container of spark-submit: `docker run --rm -it --network project-net --name spark-submit -v .:/opt/app bitnami/spark:3 /bin/bash`
    - Run next command to change directory for needed one: `cd /opt/app`
    - Run next command to start : `spark-submit --conf spark.jars.ivy=/opt/app --packages "org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,com.datastax.spark:spark-cassandra-connector_2.12:3.0.0" --master spark://spark:7077 --deploy-mode client --executor-memory 1G --total-executor-cores 2 transform_1.py`
8. Run next command: `.\RestAPI\start_api.sh` - to build and start REST API container 
9. Now you can use all endpoints from Results section to test how our application works or using commands from [commands.txt](commands.txt) to "look into" the containers of the Apache Kafka queue or Cassandra database 
10. Write `exit` - to shut down temporary container or `CTRL + C` - to shut down temporary kafka or REST API in all terminals. 
11. Run next command: `.\RestAPI\start_api.sh` - to stop and delete REST API container 
12. Run next command: `docker-compose down` - to shut down the installation.


### Results

üîª **Screenshots**  üîª
- Data in cassandra tables (intermediate)
![cassandra_table.png](screenshots/cassandra_table.png)
- Kafka Consumer which is used by spark
![kafka_con.png](screenshots/kafka_con.png)

### Endpoints

##### Precomputed:
1) Return the aggregated statistics containing the number of transactions for each cryptocurrency for each hour in the last 6 hours, excluding the previous hour.<br>
<b>Endpoint:</b> `http://127.0.0.1:1488/transactions/6-hours`
![pre_1.png](screenshots/pre_1.png)
2) Return the statistics about the total trading volume for each cryptocurrency for the last 6 hours, excluding the previous hour.<br>
<b>Endpoint:</b> `http://127.0.0.1:1488/volume/6-hours`
![pre_2.png](screenshots/pre_2.png)
3) Return aggregated statistics containing the number of trades and their total volume for each hour in the last 12 hours, excluding the current hour. <br>
<b>Endpoint:</b> `http://127.0.0.1:1488/total/12-hours`
![pre_3.png](screenshots/pre_3.png)

##### Ad_Hoc:
1) Return the number of trades processed in a specific cryptocurrency in the last N minutes, excluding the last minute.<br>
<b>Endpoint:</b> `http://127.0.0.1:1488/ETHUSD/trades/25`
![ad_1.png](screenshots/ad_1.png)
2) Return the top N cryptocurrencies with the highest trading volume in the last hour.<br>
<b>Endpoint:</b> `http://127.0.0.1:1488/trades/hour/3`
![ad_2.png](screenshots/ad_2.png) 
3) Return the cryptocurrency‚Äôs current price for ¬´Buy¬ª and ¬´Sell¬ª sides based on its symbol. <br>
<b>Endpoint:</b> `http://127.0.0.1:1488/DOGEUSD/price`
![ad_3.png](screenshots/ad_3.png)


## üìå Nota bene

1) We recommend you to use [commands.txt](commands.txt) to copy all commands, it will be a lot easier.
2) Also, the data for table `currency_price_data` is artificial, since nor os our computer were able to process writing to three tables at the same time. The code for transforming the data for this third table is commented, but you can still check it out. It works if writing process to other tables is disabled. 
3) We ran our code for 1.5 hour only before doing the screenshots. That is due to containers memory bound. I put the max value (8gb) in docker settings, but still, if run more than 1.5 hours, spark stops and cassandra container dies immediately with all written data inside.